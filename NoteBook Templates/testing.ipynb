{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torchvision import models, transforms\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import datasets\n",
    "from torchvision.io import read_image\n",
    "from torchvision.transforms import ToTensor\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">>>>>>>>>>>INTERACT HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">>>>>>>>>>>>>>>>>>RUN ALL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_transforms = {\n",
    "    'test': transforms.Compose([\n",
    "        #transforms.RandomResizedCrop(224),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.Grayscale(),\n",
    "        transforms.Grayscale(3),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ])\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def imshow(inp, title=None):\n",
    "    \"\"\"Display image for Tensor.\"\"\"\n",
    "    inp = inp.numpy().transpose((1, 2, 0))\n",
    "    mean = np.array([0.485, 0.456, 0.406])\n",
    "    std = np.array([0.229, 0.224, 0.225])\n",
    "    inp = std * inp + mean\n",
    "    inp = np.clip(inp, 0, 1)\n",
    "    plt.imshow(inp)\n",
    "    if title is not None:\n",
    "        plt.title(title)\n",
    "    plt.pause(0.001)  # pause a bit so that plots are updated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize(model, img_path):\n",
    "    img = Image.open(img_path)\n",
    "    img = img.convert('RGB')\n",
    "    img = data_transforms['test'](img)\n",
    "    img = img.unsqueeze(0)\n",
    "    img = img.to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(img)\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "\n",
    "    return class_names[preds[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_wimg(model, img_path):\n",
    "    img = Image.open(img_path)\n",
    "    img = img.convert('RGB')\n",
    "    img = data_transforms['test'](img)\n",
    "    img = img.unsqueeze(0)\n",
    "    img = img.to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(img)\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "\n",
    "        ax = plt.subplot(2,2,1)\n",
    "        ax.axis('off')\n",
    "        ax.set_title(f'Predicted: {class_names[preds[0]]}')\n",
    "        imshow(img.cpu().data[0])\n",
    "    return class_names[preds[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def confusion_matrix(model, folder_path):\n",
    "    catagories = os.listdir(folder_path)\n",
    "    if '.DS_Store' in catagories:\n",
    "        catagories.remove('.DS_Store')\n",
    "    num_catagories = len(catagories)\n",
    "    #import confusion matrix\n",
    "    value_library = {}\n",
    "    confusion_matrix = np.zeros((num_catagories, num_catagories))\n",
    "    catagory_list = os.listdir(folder_path)\n",
    "    if '.DS_Store' in catagory_list:\n",
    "        catagory_list.remove('.DS_Store')\n",
    "    for catagory in catagory_list:\n",
    "        catagory_path = os.path.join(folder_path, catagory)\n",
    "        img_list = os.listdir(catagory_path)\n",
    "        if '.DS_Store' in img_list:\n",
    "            img_list.remove('.DS_Store')\n",
    "        \n",
    "        #shuffle order of img_list\n",
    "        np.random.shuffle(img_list)\n",
    "        for img in img_list:\n",
    "            img_path = os.path.join(catagory_path, img)\n",
    "            predicted = visualize(model, img_path)\n",
    "            predicted_index = catagory_list.index(predicted)\n",
    "            actual_index = catagory_list.index(catagory)\n",
    "            if predicted_index != actual_index:\n",
    "                visualize_wimg(model,img_path)\n",
    "                print('Actual Identity: ', catagory)\n",
    "            confusion_matrix[actual_index][predicted_index] += 1\n",
    "            #write functions to calculate true positive, true negative, false positive, false negative\n",
    "    for catagory in catagory_list:\n",
    "        catagory_index = catagory_list.index(catagory)\n",
    "        true_positive = confusion_matrix[catagory_index][catagory_index]\n",
    "        false_negative = confusion_matrix[catagory_index][:].sum() - true_positive\n",
    "        num = 0\n",
    "        for row in confusion_matrix:\n",
    "            num = num + row[catagory_index]\n",
    "        false_positive = num - true_positive\n",
    "        \n",
    "\n",
    "        print(confusion_matrix[catagory_index])\n",
    "        print(confusion_matrix[0:num_catagories][catagory_index])\n",
    "        print('false_positive', false_positive)\n",
    "        print('false negative', false_negative)\n",
    "        true_negative = confusion_matrix[:][:].sum() - confusion_matrix[catagory_index][:].sum() - confusion_matrix[:][catagory_index].sum() + confusion_matrix[catagory_index][catagory_index]\n",
    "        values = {}\n",
    "        values['true_positive'] = true_positive\n",
    "        values['true_negative'] = true_negative\n",
    "        values['false_positive'] = false_positive\n",
    "        values['false_negative'] = false_negative\n",
    "        value_library[catagory] = values\n",
    "        print(confusion_matrix)\n",
    "    return value_library\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def confusion_matrix_2cat(model, folder_path):\n",
    "    num_catagories = len(os.listdir(folder_path))\n",
    "    value_library = {}\n",
    "    confusion_matrix = np.zeros((num_catagories, num_catagories))\n",
    "    catagory_list = os.listdir(folder_path)\n",
    "    for catagory in catagory_list:\n",
    "        catagory_path = os.path.join(folder_path, catagory)\n",
    "        img_list = os.listdir(catagory_path)\n",
    "        for img in img_list:\n",
    "            img_path = os.path.join(catagory_path, img)\n",
    "            predicted = visualize(model, img_path)\n",
    "            predicted_index = catagory_list.index(predicted)\n",
    "            actual_index = catagory_list.index(catagory)\n",
    "            \n",
    "            confusion_matrix[actual_index][predicted_index] += 1\n",
    "            #write functions to calculate true positive, true negative, false positive, false negative\n",
    "    for catagory in catagory_list:\n",
    "        catagory_index = catagory_list.index(catagory)\n",
    "        true_positive = confusion_matrix[catagory_index][catagory_index]\n",
    "        if catagory_index == 0:\n",
    "            false_negative = confusion_matrix[catagory_index][catagory_index+1]\n",
    "        if catagory_index == 1:\n",
    "            false_negative = confusion_matrix[catagory_index][catagory_index-1]\n",
    "        false_positive = confusion_matrix[:][catagory_index].sum() - true_positive\n",
    "        true_negative = confusion_matrix[:][:].sum() - confusion_matrix[catagory_index][:].sum() - confusion_matrix[:][catagory_index].sum() + confusion_matrix[catagory_index][catagory_index]\n",
    "        values = {}\n",
    "        values['true_positive'] = true_positive\n",
    "        values['true_negative'] = true_negative\n",
    "        values['false_positive'] = false_positive\n",
    "        values['false_negative'] = false_negative\n",
    "        value_library[catagory] = values\n",
    "    return value_library\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluation_metrics(value_library):\n",
    "    evaluation_metrics = {}\n",
    "    for catagory in value_library:\n",
    "        values = value_library[catagory]\n",
    "        true_positive = values['true_positive']\n",
    "        true_negative = values['true_negative']\n",
    "        false_positive = values['false_positive']\n",
    "        false_negative = values['false_negative']\n",
    "        accuracy = (true_positive + true_negative) / (true_positive + true_negative + false_positive + false_negative)\n",
    "        precision = true_positive / (true_positive + false_positive)\n",
    "        recall = true_positive / (true_positive + false_negative)\n",
    "        f1_score = 2 * ((precision * recall) / (precision + recall))\n",
    "        evaluation_metrics[catagory] = {'accuracy': accuracy, 'precision': precision, 'recall': recall, 'f1_score': f1_score}\n",
    "    return evaluation_metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def average_evaluation_metrics(evaluation_metrics):\n",
    "    average_evaluation_metrics = {}\n",
    "    accuracy = 0\n",
    "    precision = 0\n",
    "    recall = 0\n",
    "    f1_score = 0\n",
    "    for catagory in evaluation_metrics:\n",
    "        values = evaluation_metrics[catagory]\n",
    "        accuracy += values['accuracy']\n",
    "        precision += values['precision']\n",
    "        recall += values['recall']\n",
    "        f1_score += values['f1_score']\n",
    "    average_evaluation_metrics['accuracy'] = accuracy / len(evaluation_metrics)\n",
    "    average_evaluation_metrics['precision'] = precision / len(evaluation_metrics)\n",
    "    average_evaluation_metrics['recall'] = recall / len(evaluation_metrics)\n",
    "    average_evaluation_metrics['f1_score'] = f1_score / len(evaluation_metrics)\n",
    "    return average_evaluation_metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_eval(value_library, evaluation_metric_lib, average_evaluation_metrics_lib):\n",
    "    print('\\n' +'\\n' +'~~~~~~~~~~~~~~~~~~~~~~~EVALUATION~~~~~~~~~~~~~~~~~~~~~~~~~~~~~')\n",
    "    for value in value_library:\n",
    "        print(value + ':')\n",
    "        print(value_library[value])\n",
    "        print('catagory metrics: ')\n",
    "        print(evaluation_metric_lib[value])\n",
    "        print ('\\n')\n",
    "    print('########################################')\n",
    "    print('average evaluation metrics: ')\n",
    "    for val in average_evaluation_metrics_lib:\n",
    "        print(val + ': ' + str(average_evaluation_metrics_lib[val]))\n",
    "    print('########################################')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, folder_path, cat_num):\n",
    "    if cat_num == 2:\n",
    "        value_library = confusion_matrix_2cat(model, folder_path)\n",
    "    else:\n",
    "        value_library = confusion_matrix(model, folder_path)\n",
    "    evaluation_metric_lib = evaluation_metrics(value_library)\n",
    "    average_evaluation_metrics_lib = average_evaluation_metrics(evaluation_metric_lib)\n",
    "    visualize_eval(value_library, evaluation_metric_lib, average_evaluation_metrics_lib)\n",
    "    return average_evaluation_metrics_lib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">>>>>>>>>>>>>INTERACT HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_model = 'model.pt'\n",
    "path_to_folder = 'test.jpg'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = os.listdir(path_to_folder)\n",
    "class_names.sort()\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = torch.jit.load(path_to_model)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate(model, path_to_folder, _)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">>>>>>NEW MODEL SET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.jit.load('.pt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate(model, path_to_folder,_)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
